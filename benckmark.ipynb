{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583b0710-bfdc-476a-a183-a37a53539b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.4.0, llvm 15.0.4, commit fbe92fd8, linux, python 3.9.16\n",
      "[I 01/25/23 20:11:49.385 335178] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loyot/anaconda3/envs/torch_nightly/lib/python3.9/site-packages/taichi/types/ndarray_type.py:89: DeprecationWarning: The field_dim argument for ndarray will be deprecated in v1.5.0, use ndim instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.taichi_modules import (\n",
    "    HashEncoder, DirEncoder, HashEmbedder, SHEncoder, \n",
    "    VolumeRendererTaichi\n",
    ")\n",
    "from models.custom_functions import VolumeRenderer\n",
    "import tinycudann as tcnn\n",
    "import taichi as ti\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b70a64d-c17a-421a-b931-570c4ab6c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "ti.init(arch=ti.cuda, device_memory_GB=4, kernel_profiler=True, offline_cache=False)\n",
    "device = torch.device('cuda')\n",
    "L=16; F=2; log2_T=19; N_min=16; b=1.3195079565048218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50a0b69-573e-43ea-882e-1329fcdbf65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcnn\n",
    "cuda_hash_encoder = \\\n",
    "    tcnn.Encoding(\n",
    "        n_input_dims=3,\n",
    "        encoding_config={\n",
    "            \"otype\": \"Grid\",\n",
    "            \"type\": \"Hash\",\n",
    "            \"n_levels\": L,\n",
    "            \"n_features_per_level\": F,\n",
    "            \"log2_hashmap_size\": log2_T,\n",
    "            \"base_resolution\": N_min,\n",
    "            \"per_level_scale\": b,\n",
    "            \"interpolation\": \"Linear\"\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "cuda_dir_encoder = \\\n",
    "    tcnn.Encoding(\n",
    "        n_input_dims=3,\n",
    "        encoding_config={\n",
    "            \"otype\": \"SphericalHarmonics\",\n",
    "            \"degree\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "cuda_render_func = VolumeRenderer.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3c57c2-c6db-4e9a-9bc2-810a3e3b564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_level_scale:  1.3195079565048218\n",
      "offset_:  5722520\n"
     ]
    }
   ],
   "source": [
    "b = cuda_hash_encoder.native_tcnn_module.hyperparams()['per_level_scale']\n",
    "\n",
    "taichi_hash_encoder = HashEncoder(cuda_hash_encoder.params, b, 8192).to(device)\n",
    "taichi_dir_encoder = DirEncoder(8192).to(device)\n",
    "taichi_render_func = VolumeRendererTaichi(8192).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea84296-f364-4f58-8649-0af27690f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_hash_encoder = HashEmbedder()\n",
    "torch_dir_encoder = SHEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b644b947-6f02-49ab-a577-7330d2ab0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_rays = 10\n",
    "position = torch.load('./test_data/positions.t').float()\n",
    "dirs = torch.load('./test_data/dir.t').float()\n",
    "sigmas = torch.load('./test_data/sigmas.t').float()\n",
    "rgbs = torch.load('./test_data/rgbs.t').float()\n",
    "deltas = torch.load('./test_data/deltas.t').float()\n",
    "ts = torch.load('./test_data/ts.t').float()\n",
    "rays_a = torch.load('./test_data/rays_a.t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f806208-2703-4e30-a22f-4761c5204a17",
   "metadata": {},
   "source": [
    "## CUDA Hash\n",
    "\n",
    "CUDA total test:\n",
    "- _module_function\n",
    "- _module_function_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c24565-fcef-40ad-a399-8d445632d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        24.77%     270.000us        32.84%     358.000us     358.000us     644.000us        56.10%     645.000us     645.000us             1  \n",
      "              aten::empty         2.39%      26.000us        18.90%     206.000us     103.000us     161.000us        14.02%     161.000us      80.500us             2  \n",
      "              aten::copy_         1.65%      18.000us         3.49%      38.000us      19.000us     127.000us        11.06%     127.000us      63.500us             2  \n",
      "      aten::empty_strided         0.73%       8.000us         5.96%      65.000us      65.000us      66.000us         5.75%      66.000us      66.000us             1  \n",
      "    aten::constant_pad_nd         3.67%      40.000us        26.70%     291.000us     291.000us      32.000us         2.79%     300.000us     300.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.090ms\n",
      "Self CUDA time total: 1.148ms\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              _module_function_backward         4.67%     136.000us         5.97%     174.000us     174.000us       2.014ms        67.81%       2.134ms       2.134ms             1  \n",
      "                                              aten::mul         0.72%      21.000us         0.93%      27.000us       9.000us     204.000us         6.87%     204.000us      68.000us             3  \n",
      "                                              aten::add         0.62%      18.000us         3.54%     103.000us      51.500us     193.000us         6.50%     193.000us      96.500us             2  \n",
      "                                            aten::copy_         0.31%       9.000us         0.69%      20.000us      10.000us     152.000us         5.12%     152.000us      76.000us             2  \n",
      "                                    aten::tanh_backward         0.51%      15.000us         0.65%      19.000us      19.000us      99.000us         3.33%      99.000us      99.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.913ms\n",
      "Self CUDA time total: 2.970ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r1 = cuda_hash_encoder(position)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss1 = ((r1 * r1) - torch.tanh(r1)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss1.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b83659-22c9-4044-abd1-700374791533",
   "metadata": {},
   "source": [
    "## CUDA Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f90da-fed2-4106-bb15-98dff389c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    vr_samples, opacity, depth, r4, ws = VolumeRenderer.apply(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss4 = ((r4 * r4) - torch.tanh(r4)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss4.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688252b-616a-4a42-aebd-e3c768b6f48a",
   "metadata": {},
   "source": [
    "## Taichi\n",
    "\n",
    "CUDA total test:\n",
    "- _module_function\n",
    "- _module_function_backward\n",
    "\n",
    "CUDA total contains torch2ti and ti2torch. Use kernel_profiler instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69867865-fd2d-46bf-86a3-4fcb5a09459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        96.09%       1.866ms        99.79%       1.938ms       1.938ms       1.826ms        93.83%       1.946ms       1.946ms             1  \n",
      "              aten::fill_         0.62%      12.000us         1.34%      26.000us      26.000us      82.000us         4.21%      82.000us      82.000us             1  \n",
      "              aten::zeros         1.18%      23.000us         3.71%      72.000us      72.000us      16.000us         0.82%     120.000us     120.000us             1  \n",
      "              aten::empty         0.57%      11.000us         0.57%      11.000us      11.000us      14.000us         0.72%      14.000us      14.000us             1  \n",
      "              aten::zero_         0.62%      12.000us         1.96%      38.000us      38.000us       8.000us         0.41%      90.000us      90.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.942ms\n",
      "Self CUDA time total: 1.946ms\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               _module_functionBackward        94.71%       8.982ms        95.26%       9.034ms       9.034ms       8.304ms        86.27%       8.363ms       8.363ms             1  \n",
      "                                              aten::add         0.21%      20.000us         0.28%      27.000us      13.500us     376.000us         3.91%     376.000us     188.000us             2  \n",
      "                                              aten::mul         0.21%      20.000us         0.30%      28.000us      14.000us     255.000us         2.65%     255.000us     127.500us             2  \n",
      "                                    aten::tanh_backward         0.14%      13.000us         0.19%      18.000us      18.000us     188.000us         1.95%     188.000us     188.000us             1  \n",
      "                                             aten::add_         0.07%       7.000us         0.09%       9.000us       9.000us     158.000us         1.64%     158.000us     158.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.484ms\n",
      "Self CUDA time total: 9.626ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r3 = taichi_hash_encoder(position)\n",
    "loss3 = ((r3 * r3) - torch.tanh(r3)).sum()\n",
    "loss3.backward()\n",
    "ti.profiler.clear_kernel_profiler_info()\n",
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r3 = taichi_hash_encoder(position)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss3 = ((r3 * r3) - torch.tanh(r3)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss3.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66a574d-1674-41e8-a4d7-4b63b535e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================\n",
      "Kernel Profiler(trace, default) @ CUDA on NVIDIA GeForce RTX 3090 Ti\n",
      "=======================================================================================================\n",
      "[  start.time | kernel.time |   regs  |   shared mem | grid size | block size | occupancy ] Kernel name\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "[    0.000 ms |    0.012 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_0_kernel_0_serial\n",
      "[    0.012 ms |    0.020 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_0_kernel_1_range_for\n",
      "[    0.032 ms |    0.008 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_1_kernel_0_serial\n",
      "[    0.040 ms |    0.111 ms |      15 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_1_kernel_1_range_for\n",
      "[    0.151 ms |    0.007 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] hash_encode_kernel_c104_0_kernel_0_serial\n",
      "[    0.158 ms |    0.987 ms |      40 |      0 bytes |      2688 |         16 | 16 blocks ] hash_encode_kernel_c104_0_kernel_1_range_for\n",
      "[    1.146 ms |    0.007 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_0_kernel_0_serial\n",
      "[    1.153 ms |    0.135 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_0_kernel_1_range_for\n",
      "[    1.288 ms |    0.053 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_0_kernel_0_range_for\n",
      "[    1.341 ms |    0.009 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_0_kernel_0_serial\n",
      "[    1.350 ms |    0.138 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_0_kernel_1_range_for\n",
      "[    1.488 ms |    0.007 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] hash_encode_kernel_c105_0_reverse_grad_reverse_grad_kernel_0_serial\n",
      "[    1.495 ms |    7.561 ms |      80 |      0 bytes |      2688 |         16 | 16 blocks ] hash_encode_kernel_c105_0_reverse_grad_reverse_grad_kernel_1_range_for\n",
      "[    9.056 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_0_kernel_0_serial\n",
      "[    9.060 ms |    0.110 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_0_kernel_1_range_for\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Number of records:  15\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "ti.profiler.print_kernel_profiler_info('trace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee970b-c6f2-4fb7-a9d0-32f39cf09e71",
   "metadata": {},
   "source": [
    "## Taichi Volume\n",
    "\n",
    "- composite_train_fw_c92_0_kernel_0_range_for\n",
    "- composite_train_fw_x_reverse_grad_reverse_grad_kernel_0_range_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85e5774-dd6d-48a4-8571-32abee1450e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================\n",
      "Kernel Profiler(trace, default) @ CUDA on NVIDIA GeForce RTX 3090 Ti\n",
      "=======================================================================================================\n",
      "[  start.time | kernel.time |   regs  |   shared mem | grid size | block size | occupancy ] Kernel name\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "[    0.000 ms |    0.006 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_0_kernel_0_serial\n",
      "[    0.006 ms |    0.008 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_0_kernel_1_range_for\n",
      "[    0.014 ms |    0.004 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_1_kernel_1_serial\n",
      "[    0.018 ms |    0.016 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_1_kernel_0_range_for\n",
      "[    0.034 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_2_kernel_0_serial\n",
      "[    0.038 ms |    0.009 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_2_kernel_1_range_for\n",
      "[    0.046 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_3_kernel_0_serial\n",
      "[    0.050 ms |    0.008 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_3_kernel_1_range_for\n",
      "[    0.058 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_4_kernel_0_serial\n",
      "[    0.061 ms |    0.005 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_4_kernel_1_range_for\n",
      "[    0.067 ms |    0.727 ms |      44 |      0 bytes |        32 |        256 |  5 blocks ] composite_train_fw_c92_0_kernel_0_range_for\n",
      "[    0.794 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_0_kernel_0_serial\n",
      "[    0.798 ms |    0.005 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_0_kernel_1_range_for\n",
      "[    0.803 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_1_kernel_0_serial\n",
      "[    0.807 ms |    0.006 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_1_kernel_1_range_for\n",
      "[    0.813 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_2_kernel_0_serial\n",
      "[    0.816 ms |    0.005 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_2_kernel_1_range_for\n",
      "[    0.822 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_3_kernel_0_serial\n",
      "[    0.825 ms |    0.005 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_3_kernel_1_range_for\n",
      "[    0.830 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_4_kernel_0_serial\n",
      "[    0.833 ms |    0.008 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_4_kernel_1_range_for\n",
      "[    0.841 ms |    0.038 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_0_kernel_0_range_for\n",
      "[    0.879 ms |    0.109 ms |      10 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_1_kernel_0_range_for\n",
      "[    0.988 ms |    0.036 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_2_kernel_0_range_for\n",
      "[    1.025 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_0_kernel_0_serial\n",
      "[    1.028 ms |    0.005 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_0_kernel_1_range_for\n",
      "[    1.034 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_1_kernel_0_serial\n",
      "[    1.038 ms |    0.005 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_1_kernel_1_range_for\n",
      "[    1.043 ms |    0.004 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_2_kernel_0_serial\n",
      "[    1.046 ms |    0.005 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_2_kernel_1_range_for\n",
      "[    1.052 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_3_kernel_0_serial\n",
      "[    1.055 ms |    0.009 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_3_kernel_1_range_for\n",
      "[    1.064 ms |    1.386 ms |      64 |      0 bytes |        32 |        256 |  4 blocks ] composite_train_fw_c93_0_reverse_grad_reverse_grad_kernel_0_range_for\n",
      "[    2.449 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_0_kernel_0_serial\n",
      "[    2.453 ms |    0.008 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_0_kernel_1_range_for\n",
      "[    2.461 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_1_kernel_0_serial\n",
      "[    2.464 ms |    0.016 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_1_kernel_1_range_for\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Number of records:  37\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "vr_samples, opacity, depth, r5, ws = taichi_render_func(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "loss5 = ((r5 * r5) - torch.tanh(r5)).sum()\n",
    "loss5.backward()\n",
    "ti.profiler.clear_kernel_profiler_info()\n",
    "\n",
    "vr_samples, opacity, depth, r5, ws = taichi_render_func(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "loss5 = ((r5 * r5) - torch.tanh(r5)).sum()\n",
    "loss5.backward()\n",
    "\n",
    "ti.profiler.print_kernel_profiler_info('trace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_nightly]",
   "language": "python",
   "name": "conda-env-torch_nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
