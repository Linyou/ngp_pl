{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583b0710-bfdc-476a-a183-a37a53539b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.4.0, llvm 15.0.4, commit fbe92fd8, linux, python 3.9.16\n",
      "[I 01/25/23 20:11:49.385 335178] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loyot/anaconda3/envs/torch_nightly/lib/python3.9/site-packages/taichi/types/ndarray_type.py:89: DeprecationWarning: The field_dim argument for ndarray will be deprecated in v1.5.0, use ndim instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.taichi_modules import (\n",
    "    HashEncoder, DirEncoder, HashEmbedder, SHEncoder, \n",
    "    VolumeRendererTaichi\n",
    ")\n",
    "from models.custom_functions import VolumeRenderer\n",
    "import tinycudann as tcnn\n",
    "import taichi as ti\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d38178-76b9-4f34-8b42-a2ee13b8a7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = int(img.shape[1] / 4)\n",
    "height = int(img.shape[0] / 4)\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(img, dim)\n",
    "cv2.imwrite('/home/loyot/workspace/Datasets/NeRF/360_v2/bicycle/images_4/_DSC8745.JPG', resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b70a64d-c17a-421a-b931-570c4ab6c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "ti.init(arch=ti.cuda, device_memory_GB=4, kernel_profiler=True, offline_cache=False)\n",
    "device = torch.device('cuda')\n",
    "L=16; F=2; log2_T=19; N_min=16; b=1.3195079565048218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50a0b69-573e-43ea-882e-1329fcdbf65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcnn\n",
    "cuda_hash_encoder = \\\n",
    "    tcnn.Encoding(\n",
    "        n_input_dims=3,\n",
    "        encoding_config={\n",
    "            \"otype\": \"Grid\",\n",
    "            \"type\": \"Hash\",\n",
    "            \"n_levels\": L,\n",
    "            \"n_features_per_level\": F,\n",
    "            \"log2_hashmap_size\": log2_T,\n",
    "            \"base_resolution\": N_min,\n",
    "            \"per_level_scale\": b,\n",
    "            \"interpolation\": \"Linear\"\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "cuda_dir_encoder = \\\n",
    "    tcnn.Encoding(\n",
    "        n_input_dims=3,\n",
    "        encoding_config={\n",
    "            \"otype\": \"SphericalHarmonics\",\n",
    "            \"degree\": 4,\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "cuda_render_func = VolumeRenderer.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3c57c2-c6db-4e9a-9bc2-810a3e3b564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_level_scale:  1.3195079565048218\n",
      "offset_:  5722520\n"
     ]
    }
   ],
   "source": [
    "b = cuda_hash_encoder.native_tcnn_module.hyperparams()['per_level_scale']\n",
    "\n",
    "taichi_hash_encoder = HashEncoder(cuda_hash_encoder.params, b, 8192).to(device)\n",
    "taichi_dir_encoder = DirEncoder(8192).to(device)\n",
    "taichi_render_func = VolumeRendererTaichi(8192).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea84296-f364-4f58-8649-0af27690f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_hash_encoder = HashEmbedder()\n",
    "torch_dir_encoder = SHEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b644b947-6f02-49ab-a577-7330d2ab0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_rays = 10\n",
    "position = torch.load('./test_data/positions.t').float()\n",
    "dirs = torch.load('./test_data/dir.t').float()\n",
    "sigmas = torch.load('./test_data/sigmas.t').float()\n",
    "rgbs = torch.load('./test_data/rgbs.t').float()\n",
    "deltas = torch.load('./test_data/deltas.t').float()\n",
    "ts = torch.load('./test_data/ts.t').float()\n",
    "rays_a = torch.load('./test_data/rays_a.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2711c30d-46d1-4cdf-891c-7035b7ef7dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([440506, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506389fe-e11a-46c7-b5aa-a35201010f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([440506])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fa6466-2166-462c-9450-a1609065e99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.044999999999998"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22.136 + 5.909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c24565-fcef-40ad-a399-8d445632d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        24.77%     270.000us        32.84%     358.000us     358.000us     644.000us        56.10%     645.000us     645.000us             1  \n",
      "              aten::empty         2.39%      26.000us        18.90%     206.000us     103.000us     161.000us        14.02%     161.000us      80.500us             2  \n",
      "              aten::copy_         1.65%      18.000us         3.49%      38.000us      19.000us     127.000us        11.06%     127.000us      63.500us             2  \n",
      "      aten::empty_strided         0.73%       8.000us         5.96%      65.000us      65.000us      66.000us         5.75%      66.000us      66.000us             1  \n",
      "    aten::constant_pad_nd         3.67%      40.000us        26.70%     291.000us     291.000us      32.000us         2.79%     300.000us     300.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.090ms\n",
      "Self CUDA time total: 1.148ms\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              _module_function_backward         4.67%     136.000us         5.97%     174.000us     174.000us       2.014ms        67.81%       2.134ms       2.134ms             1  \n",
      "                                              aten::mul         0.72%      21.000us         0.93%      27.000us       9.000us     204.000us         6.87%     204.000us      68.000us             3  \n",
      "                                              aten::add         0.62%      18.000us         3.54%     103.000us      51.500us     193.000us         6.50%     193.000us      96.500us             2  \n",
      "                                            aten::copy_         0.31%       9.000us         0.69%      20.000us      10.000us     152.000us         5.12%     152.000us      76.000us             2  \n",
      "                                    aten::tanh_backward         0.51%      15.000us         0.65%      19.000us      19.000us      99.000us         3.33%      99.000us      99.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.913ms\n",
      "Self CUDA time total: 2.970ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r1 = cuda_hash_encoder(position)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss1 = ((r1 * r1) - torch.tanh(r1)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss1.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fef0526-fc13-4584-9ffd-2f54b70a0c1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_hash_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check forward\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[0;32m----> 3\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m \u001b[43mpytorch_hash_encoder\u001b[49m(position)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch forward\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, prof\u001b[38;5;241m.\u001b[39mkey_averages(group_by_stack_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mtable(\n\u001b[1;32m      5\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_cuda_time_total\u001b[39m\u001b[38;5;124m'\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# check backward\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# a strange loss for better verification\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_hash_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r2 = pytorch_hash_encoder(position)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss2 = ((r2 * r2) - torch.tanh(r2)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss2.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69867865-fd2d-46bf-86a3-4fcb5a09459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        96.09%       1.866ms        99.79%       1.938ms       1.938ms       1.826ms        93.83%       1.946ms       1.946ms             1  \n",
      "              aten::fill_         0.62%      12.000us         1.34%      26.000us      26.000us      82.000us         4.21%      82.000us      82.000us             1  \n",
      "              aten::zeros         1.18%      23.000us         3.71%      72.000us      72.000us      16.000us         0.82%     120.000us     120.000us             1  \n",
      "              aten::empty         0.57%      11.000us         0.57%      11.000us      11.000us      14.000us         0.72%      14.000us      14.000us             1  \n",
      "              aten::zero_         0.62%      12.000us         1.96%      38.000us      38.000us       8.000us         0.41%      90.000us      90.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.942ms\n",
      "Self CUDA time total: 1.946ms\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               _module_functionBackward        94.71%       8.982ms        95.26%       9.034ms       9.034ms       8.304ms        86.27%       8.363ms       8.363ms             1  \n",
      "                                              aten::add         0.21%      20.000us         0.28%      27.000us      13.500us     376.000us         3.91%     376.000us     188.000us             2  \n",
      "                                              aten::mul         0.21%      20.000us         0.30%      28.000us      14.000us     255.000us         2.65%     255.000us     127.500us             2  \n",
      "                                    aten::tanh_backward         0.14%      13.000us         0.19%      18.000us      18.000us     188.000us         1.95%     188.000us     188.000us             1  \n",
      "                                             aten::add_         0.07%       7.000us         0.09%       9.000us       9.000us     158.000us         1.64%     158.000us     158.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.484ms\n",
      "Self CUDA time total: 9.626ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r3 = taichi_hash_encoder(position)\n",
    "loss3 = ((r3 * r3) - torch.tanh(r3)).sum()\n",
    "loss3.backward()\n",
    "ti.profiler.clear_kernel_profiler_info()\n",
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r3 = taichi_hash_encoder(position)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss3 = ((r3 * r3) - torch.tanh(r3)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss3.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66a574d-1674-41e8-a4d7-4b63b535e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================\n",
      "Kernel Profiler(trace, default) @ CUDA on NVIDIA GeForce RTX 3090 Ti\n",
      "=======================================================================================================\n",
      "[  start.time | kernel.time |   regs  |   shared mem | grid size | block size | occupancy ] Kernel name\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "[    0.000 ms |    0.012 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_0_kernel_0_serial\n",
      "[    0.012 ms |    0.020 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_0_kernel_1_range_for\n",
      "[    0.032 ms |    0.008 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_1_kernel_0_serial\n",
      "[    0.040 ms |    0.111 ms |      15 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_1_kernel_1_range_for\n",
      "[    0.151 ms |    0.007 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] hash_encode_kernel_c104_0_kernel_0_serial\n",
      "[    0.158 ms |    0.987 ms |      40 |      0 bytes |      2688 |         16 | 16 blocks ] hash_encode_kernel_c104_0_kernel_1_range_for\n",
      "[    1.146 ms |    0.007 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_0_kernel_0_serial\n",
      "[    1.153 ms |    0.135 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_0_kernel_1_range_for\n",
      "[    1.288 ms |    0.053 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_0_kernel_0_range_for\n",
      "[    1.341 ms |    0.009 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_0_kernel_0_serial\n",
      "[    1.350 ms |    0.138 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_0_kernel_1_range_for\n",
      "[    1.488 ms |    0.007 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] hash_encode_kernel_c105_0_reverse_grad_reverse_grad_kernel_0_serial\n",
      "[    1.495 ms |    7.561 ms |      80 |      0 bytes |      2688 |         16 | 16 blocks ] hash_encode_kernel_c105_0_reverse_grad_reverse_grad_kernel_1_range_for\n",
      "[    9.056 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_0_kernel_0_serial\n",
      "[    9.060 ms |    0.110 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_0_kernel_1_range_for\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Number of records:  15\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "ti.profiler.print_kernel_profiler_info('trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd57dc33-8c8a-4e93-831c-57dc6d507216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           VolumeRenderer        74.12%     524.000us        98.87%     699.000us     699.000us     490.000us        76.68%     639.000us     639.000us             1  \n",
      "              aten::fill_         3.54%      25.000us         6.65%      47.000us       9.400us      55.000us         8.61%      55.000us      11.000us             5  \n",
      "              aten::zeros         6.36%      45.000us        19.38%     137.000us      27.400us      33.000us         5.16%     140.000us      28.000us             5  \n",
      "              aten::empty         3.11%      22.000us         3.11%      22.000us       4.400us      30.000us         4.69%      30.000us       6.000us             5  \n",
      "              aten::zero_         3.25%      23.000us         9.90%      70.000us      14.000us      22.000us         3.44%      77.000us      15.400us             5  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 707.000us\n",
      "Self CUDA time total: 639.000us\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 VolumeRendererBackward         9.10%      65.000us        23.81%     170.000us     170.000us     432.000us        55.67%     545.000us     545.000us             1  \n",
      "                                            aten::fill_         1.96%      14.000us         3.36%      24.000us       3.429us      40.000us         5.15%      40.000us       5.714us             7  \n",
      "                                            aten::zeros         6.30%      45.000us        13.73%      98.000us      16.333us      31.000us         3.99%     102.000us      17.000us             6  \n",
      "                                              aten::neg         2.24%      16.000us         3.36%      24.000us      24.000us      27.000us         3.48%      27.000us      27.000us             1  \n",
      "                                              aten::mul         1.82%      13.000us         2.38%      17.000us       5.667us      26.000us         3.35%      26.000us       8.667us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 714.000us\n",
      "Self CUDA time total: 776.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    vr_samples, opacity, depth, r4, ws = VolumeRenderer.apply(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss4 = ((r4 * r4) - torch.tanh(r4)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss4.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85e5774-dd6d-48a4-8571-32abee1450e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================\n",
      "Kernel Profiler(trace, default) @ CUDA on NVIDIA GeForce RTX 3090 Ti\n",
      "=======================================================================================================\n",
      "[  start.time | kernel.time |   regs  |   shared mem | grid size | block size | occupancy ] Kernel name\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "[    0.000 ms |    0.006 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_0_kernel_0_serial\n",
      "[    0.006 ms |    0.008 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_0_kernel_1_range_for\n",
      "[    0.014 ms |    0.004 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_1_kernel_1_serial\n",
      "[    0.018 ms |    0.016 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_1_kernel_0_range_for\n",
      "[    0.034 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_2_kernel_0_serial\n",
      "[    0.038 ms |    0.009 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_2_kernel_1_range_for\n",
      "[    0.046 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_3_kernel_0_serial\n",
      "[    0.050 ms |    0.008 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_3_kernel_1_range_for\n",
      "[    0.058 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_c82_4_kernel_0_serial\n",
      "[    0.061 ms |    0.005 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_c82_4_kernel_1_range_for\n",
      "[    0.067 ms |    0.727 ms |      44 |      0 bytes |        32 |        256 |  5 blocks ] composite_train_fw_c92_0_kernel_0_range_for\n",
      "[    0.794 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_0_kernel_0_serial\n",
      "[    0.798 ms |    0.005 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_0_kernel_1_range_for\n",
      "[    0.803 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_1_kernel_0_serial\n",
      "[    0.807 ms |    0.006 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_1_kernel_1_range_for\n",
      "[    0.813 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_2_kernel_0_serial\n",
      "[    0.816 ms |    0.005 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_2_kernel_1_range_for\n",
      "[    0.822 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_3_kernel_0_serial\n",
      "[    0.825 ms |    0.005 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_3_kernel_1_range_for\n",
      "[    0.830 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_c84_4_kernel_0_serial\n",
      "[    0.833 ms |    0.008 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_c84_4_kernel_1_range_for\n",
      "[    0.841 ms |    0.038 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_0_kernel_0_range_for\n",
      "[    0.879 ms |    0.109 ms |      10 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_1_kernel_0_range_for\n",
      "[    0.988 ms |    0.036 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] fill_tensor_c0_2_kernel_0_range_for\n",
      "[    1.025 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_0_kernel_0_serial\n",
      "[    1.028 ms |    0.005 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_0_kernel_1_range_for\n",
      "[    1.034 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_1_kernel_0_serial\n",
      "[    1.038 ms |    0.005 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_1_kernel_1_range_for\n",
      "[    1.043 ms |    0.004 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_2_kernel_0_serial\n",
      "[    1.046 ms |    0.005 ms |      18 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_2_kernel_1_range_for\n",
      "[    1.052 ms |    0.003 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] torch2ti_grad_c88_3_kernel_0_serial\n",
      "[    1.055 ms |    0.009 ms |      16 |      0 bytes |      2688 |        128 | 12 blocks ] torch2ti_grad_c88_3_kernel_1_range_for\n",
      "[    1.064 ms |    1.386 ms |      64 |      0 bytes |        32 |        256 |  4 blocks ] composite_train_fw_c93_0_reverse_grad_reverse_grad_kernel_0_range_for\n",
      "[    2.449 ms |    0.004 ms |       8 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_0_kernel_0_serial\n",
      "[    2.453 ms |    0.008 ms |      17 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_0_kernel_1_range_for\n",
      "[    2.461 ms |    0.003 ms |      18 |      0 bytes |         1 |          1 | 16 blocks ] ti2torch_grad_c86_1_kernel_0_serial\n",
      "[    2.464 ms |    0.016 ms |      20 |      0 bytes |      2688 |        128 | 12 blocks ] ti2torch_grad_c86_1_kernel_1_range_for\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Number of records:  37\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "vr_samples, opacity, depth, r5, ws = taichi_render_func(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "loss5 = ((r5 * r5) - torch.tanh(r5)).sum()\n",
    "loss5.backward()\n",
    "ti.profiler.clear_kernel_profiler_info()\n",
    "\n",
    "vr_samples, opacity, depth, r5, ws = taichi_render_func(sigmas, rgbs, deltas, ts, rays_a, 1e-4)\n",
    "loss5 = ((r5 * r5) - torch.tanh(r5)).sum()\n",
    "loss5.backward()\n",
    "\n",
    "ti.profiler.print_kernel_profiler_info('trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44979241-08a7-41c8-9da2-6618dbfb524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        20.35%     116.000us        24.21%     138.000us     138.000us     240.000us        38.16%     245.000us     245.000us             1  \n",
      "              aten::empty         4.21%      24.000us        27.02%     154.000us      77.000us     159.000us        25.28%     159.000us      79.500us             2  \n",
      "              aten::fill_         2.63%      15.000us         5.09%      29.000us      29.000us     109.000us        17.33%     109.000us     109.000us             1  \n",
      "    aten::constant_pad_nd        10.35%      59.000us        54.56%     311.000us     311.000us      38.000us         6.04%     324.000us     324.000us             1  \n",
      "                aten::pad         4.21%      24.000us        58.77%     335.000us     335.000us      24.000us         3.82%     348.000us     348.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 570.000us\n",
      "Self CUDA time total: 629.000us\n",
      "\n",
      "pytorch backward\n",
      " -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::add         2.28%      14.000us        15.61%      96.000us      48.000us     106.000us        16.59%     106.000us      53.000us             2  \n",
      "                                              aten::mul         2.60%      16.000us         3.41%      21.000us       7.000us     102.000us        15.96%     102.000us      34.000us             3  \n",
      "                                    aten::tanh_backward         2.60%      16.000us         3.25%      20.000us      20.000us      64.000us        10.02%      64.000us      64.000us             1  \n",
      "                                              aten::neg         3.58%      22.000us         5.04%      31.000us      31.000us      43.000us         6.73%      43.000us      43.000us             1  \n",
      "                              _module_function_backward        16.26%     100.000us        20.49%     126.000us     126.000us      42.000us         6.57%      89.000us      89.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 615.000us\n",
      "Self CUDA time total: 639.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:300] Completed Stage: Warm Up\n",
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:306] Completed Stage: Collection\n",
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:310] Completed Stage: Post Processing\n",
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:300] Completed Stage: Warm Up\n",
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:306] Completed Stage: Collection\n",
      "STAGE:2023-01-25 20:14:00 335178:335178 ActivityProfilerController.cpp:310] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r6 = cuda_dir_encoder(dirs)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss6 = ((r6 * r6) - torch.tanh(r6)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss6.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebdfbc3-566b-4ed0-b2fa-3d5783d60a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch forward\n",
      " -------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _module_function        98.00%       3.628ms        99.78%       3.694ms       3.694ms       3.606ms        98.82%       3.649ms       3.649ms             1  \n",
      "              aten::fill_         0.43%      16.000us         0.84%      31.000us      31.000us      35.000us         0.96%      35.000us      35.000us             1  \n",
      "              aten::zeros         0.38%      14.000us         1.78%      66.000us      66.000us       5.000us         0.14%      43.000us      43.000us             1  \n",
      "              aten::zero_         0.22%       8.000us         1.05%      39.000us      39.000us       2.000us         0.05%      37.000us      37.000us             1  \n",
      "              aten::empty         0.35%      13.000us         0.35%      13.000us      13.000us       1.000us         0.03%       1.000us       1.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.702ms\n",
      "Self CUDA time total: 3.649ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:300] Completed Stage: Warm Up\n",
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:306] Completed Stage: Collection\n",
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:310] Completed Stage: Post Processing\n",
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:300] Completed Stage: Warm Up\n",
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:306] Completed Stage: Collection\n",
      "STAGE:2023-01-25 20:14:47 335178:335178 ActivityProfilerController.cpp:310] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss7 \u001b[38;5;241m=\u001b[39m ((r7 \u001b[38;5;241m*\u001b[39m r7) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(r7))\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mloss7\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch backward\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, prof\u001b[38;5;241m.\u001b[39mkey_averages(group_by_stack_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mtable(\n\u001b[1;32m     13\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_cuda_time_total\u001b[39m\u001b[38;5;124m'\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_nightly/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_nightly/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# check forward\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    r7 = taichi_dir_encoder(dirs)\n",
    "print('pytorch forward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))\n",
    "\n",
    "# check backward\n",
    "# a strange loss for better verification\n",
    "loss7 = ((r7 * r7) - torch.tanh(r7)).sum()\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    loss7.backward()\n",
    "print('pytorch backward\\n', prof.key_averages(group_by_stack_n=5).table(\n",
    "    sort_by='self_cuda_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ad8d4-2cc1-49f9-a379-fbb70c79a741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_nightly]",
   "language": "python",
   "name": "conda-env-torch_nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
